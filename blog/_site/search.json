[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portafolio trabajos",
    "section": "",
    "text": "Primer trabajo: Optimización heurística\n\n\n\n\n\n\noptimización\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\nCatalina Restrepo Salgado, Julián Castaño Pineda, Tomás Rodríguez Taborda, Luis Andrés Altamar Romero\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 12, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Primer trabajo: Optimización heurística",
    "section": "",
    "text": "El objetivo de esta sección es evaluar diversos métodos de optimización aplicados a varias funciones, con el fin de medir su rendimiento. En particular, se utilizarán las funciones de Rosenbrock, Schwefel, Griewank, Goldstein-Price y la función de las seis jorobas de camello. Estas funciones serán optimizadas mediante el método del gradiente descendente y tres algoritmos heurísticos: Algoritmos Evolutivos, Optimización por Enjambre de Partículas y Evolución Diferencial.\nAl final, se comentará sobre los aportes de los métodos de descenso por gradiente y los métodos heurísticos, considerando el valor final de la función objetivo y el número de evaluaciones de la función objetivo, en un entorno de simulación con varios parámetros y condiciones para garantizar conclusiones significativas.\n\n\n\nFunción de RosenbrockFunción de RastriginFunción de SchwefelFunción de GriewankFunción Goldstein-PriceFunción de las seis jorobas de camello\n\n\n\\[f(\\mathbf{x}) = \\sum_{i=1}^{d-1} \\left[ 100(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \\right]\\]\nEn 2 dimensiones se puede definir como \\[ f(x_1, x_2) = (a - x_1)^2 + b(x_2 - x_1^2)^2 \\]\nLa Función de Rosenbrock, también conocida como función del valle o del plátano, es ampliamente utilizada para evaluar algoritmos de optimización basados en gradientes. Esta función es unimodal y presenta su mínimo global en un valle parabólico estrecho, lo que facilita su localización. Sin embargo, segun Simon Fraser University (n.d.) citando a Picheny, Wagner, and Ginsbourger (2012) convergencia hacia este mínimo puede ser desafiante debido a la naturaleza del valle.\nLa función se evalúa generalmente en el hipercubo \\(x_i \\in [-5, 10]\\) y tiene un mínimo global en \\(f(1,...,1) = 0\\)\n\n\n\\[f(\\mathbf{x}) = 10d + \\sum_{i=1}^{d} \\left[ x_i^2 - 10 \\cos(2\\pi x_i) \\right]\\]\nSegun (simonfraser_rosenbroc?), la función de Rastrigin tiene varios mínimos locales. Es altamente multimodal, pero las ubicaciones de los mínimos se distribuyen regularmente. La función generalmente se evalúa en el hipercubo \\(x_i \\in [-5.12, 5.12]\\) y su mínimo local se encuentra en \\(f(0,...,0)=0\\).\n\n\n\\[ f(\\mathbf{x}) = 418.9829n - \\sum_{i=1}^{n} x_i \\sin(\\sqrt{|x_i|}) \\]\nSegun Simon Fraser University (n.d.) La función de Schwefel es compleja, con muchos mínimos locales. Normalmente se evalua en el hipercubo $x_i $. Su minimo global está en \\(f(420.9687,...,420.9687)=0\\)\n\n\n\\[ f(\\mathbf{x}) = 1 + \\frac{1}{4000} \\sum_{i=1}^{n} x_i^2 - \\prod_{i=1}^{n} \\cos\\left(\\frac{x_i}{\\sqrt{i}}\\right) \\]\nSegun Simon Fraser University (n.d.) la función de Griewank tiene muchos mínimos locales generalizados, que se distribuyen de forma regular. Lo que hace compleja su optimización al minimo global. Normalmente se evalua en el hipercubo $x_i $. Su minimo global está en \\(f(0,...,0)=0\\)\n\n\n\\[\n\\begin{align}\nf(x_1, x_2) = & \\left[1 + (x_1 + x_2 + 1)^2 (19 - 14x_1 + 3x_1^2 - 14x_2 + 6x_1x_2 + 3x_2^2)\\right] \\\\\n         & \\left[30 + (2x_1 - 3x_2)^2 (18 - 32x_1 + 12x_1^2 + 48x_2 - 36x_1x_2 + 27x_2^2)\\right]\n\\end{align}\n\\]\nLa función Goldstein-Price es una función en 2 dimensiones y tiene varios mínimos locales. Segun Molga and Smutnicki (2005), la función generalmente se evalúa en el cuadrado \\(x_1 \\in [-2, 2]\\) y \\(x_1 \\in [-2, 2]\\) . Su mínimo global es \\(f(0,-1) = 3\\)\n\n\n\\[ f(x_1, x_2) = \\left(4 - 2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2 \\]\nLa función de las seis jorobas de camello es una función en 2 dimensiones.Segun Molga and Smutnicki (2005) la función tiene seis mínimos locales, dos de los cuales son globales y recomienda evaluar la función en el rectángulo \\(x_1 \\in [-3, 3], x_2 \\in [-2, 2]\\), donde los mínimos globales son \\(f(0.0898,-0.7126) = -1.0316\\) y \\(f(-0.0898, 0.7126) = -1.0316\\)\n\n\n\n\n\n\n\n\n\nFunción de RosenbrockFunción de SchwefelFunción de GriewankFunción Goldstein-PriceFunción de las seis jorobas de camello\n\n\n\\[ f(x_1, x_2) = (a - x_1)^2 + b(x_2 - x_1^2)^2 \\]\n\n\n\\[ f(x_1,x_2) = 418.9829n - \\sum_{i=1}^{2} x_i \\sin(\\sqrt{|x_i|}) \\]\n\n\n\\[ f(x_1,x_2) = 1 + \\frac{1}{4000} \\sum_{i=1}^{2} x_i^2 - \\prod_{i=1}^{2} \\cos\\left(\\frac{x_i}{\\sqrt{i}}\\right) \\]\n\n\n\\[\n\\begin{align}\nf(x_1, x_2) = & \\left[1 + (x_1 + x_2 + 1)^2 (19 - 14x_1 + 3x_1^2 - 14x_2 + 6x_1x_2 + 3x_2^2)\\right] \\\\\n         & \\left[30 + (2x_1 - 3x_2)^2 (18 - 32x_1 + 12x_1^2 + 48x_2 - 36x_1x_2 + 27x_2^2)\\right]\n\\end{align}\n\\]\n\n\n\\[ f(x_1, x_2) = \\left(4 - 2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2 \\]\n\n\n\n\n\n\n\nEscoja dos funciones de prueba.\nOptimización con método de descenso por gradiente:\n\nOptimice las funciones seleccionadas en dos y tres dimensiones usando un método de descenso por gradiente con condición inicial aleatoria.\n\nOptimización con métodos heurísticos:\n\nOptimice las funciones seleccionadas en dos y tres dimensiones usando:\n\nAlgoritmos evolutivos.\nOptimización de partículas.\nEvolución diferencial.\n\n\nRepresentación visual:\n\nCree un GIF animado o un video que muestre el proceso de optimización usando:\n\nDescenso por gradiente.\nMétodos heurísticos.\n\n\n\n\n\n\nReflexione sobre los siguientes puntos: - ¿Qué aportaron los métodos de descenso por gradiente y qué aportaron los métodos heurísticos? - Para responder a esta pregunta, considere: - El valor final de la función objetivo. - El número de evaluaciones de la función objetivo. - Es posible que se requiera realizar varias corridas de los algoritmos para obtener conclusiones significativas."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html#considere-las-siguientes-funciones-de-prueba",
    "href": "posts/post-with-code/index.html#considere-las-siguientes-funciones-de-prueba",
    "title": "Primer trabajo: Optimización heurística",
    "section": "",
    "text": "Función de Rosenbrock \\[ f(x, y) = (a - x)^2 + b(y - x^2)^2 \\]\nFunción de Rastrigin \\[ f(\\mathbf{x}) = An + \\sum_{i=1}^{n} \\left[ x_i^2 - A \\cos(2 \\pi x_i) \\right] \\]\nFunción de Schwefel \\[ f(\\mathbf{x}) = 418.9829n - \\sum_{i=1}^{n} x_i \\sin(\\sqrt{|x_i|}) \\]\nFunción de Griewank \\[ f(\\mathbf{x}) = 1 + \\frac{1}{4000} \\sum_{i=1}^{n} x_i^2 - \\prod_{i=1}^{n} \\cos\\left(\\frac{x_i}{\\sqrt{i}}\\right) \\]\nFunción Goldstein-Price \\[ f(x, y) = \\left[1 + (x + y + 1)^2 (19 - 14x + 3x^2 - 14y + 6xy + 3y^2)\\right] \\left[30 + (2x - 3y)^2 (18 - 32x + 12x^2 + 48y - 36xy + 27y^2)\\right] \\]\nFunción de las seis jorobas de camello \\[ f(x, y) = \\left(4 - 2.1x^2 + \\frac{x^4}{3}\\right)x^2 + xy + \\left(-4 + 4y^2\\right)y^2 \\]\n\n\n\n\nEscoja dos funciones de prueba.\nOptimización con método de descenso por gradiente:\n\nOptimice las funciones seleccionadas en dos y tres dimensiones usando un método de descenso por gradiente con condición inicial aleatoria.\n\nOptimización con métodos heurísticos:\n\nOptimice las funciones seleccionadas en dos y tres dimensiones usando:\n\nAlgoritmos evolutivos.\nOptimización de partículas.\nEvolución diferencial.\n\n\nRepresentación visual:\n\nCree un GIF animado o un video que muestre el proceso de optimización usando:\n\nDescenso por gradiente.\nMétodos heurísticos.\n\n\n\n\n\n\nReflexione sobre los siguientes puntos: - ¿Qué aportaron los métodos de descenso por gradiente y qué aportaron los métodos heurísticos? - Para responder a esta pregunta, considere: - El valor final de la función objetivo. - El número de evaluaciones de la función objetivo. - Es posible que se requiera realizar varias corridas de los algoritmos para obtener conclusiones significativas."
  },
  {
    "objectID": "posts/post-with-code/index.html#problema-del-viajero",
    "href": "posts/post-with-code/index.html#problema-del-viajero",
    "title": "Primer trabajo: Optimización heurística",
    "section": "Problema del Viajero:",
    "text": "Problema del Viajero:\nUn vendedor debe realizar un recorrido por todas las capitales de los 32 estados de los Estados Unidos Mexicanos.\n\nTareas:\n\nOptimización con métodos metaheurísticos:\n\nUtilice colonias de hormigas para encontrar el orden óptimo del recorrido.\nUtilice algoritmos genéticos para encontrar el orden óptimo del recorrido.\n\nCosto del recorrido:\n\nEl costo de desplazamiento entre ciudades se calcula como la suma de:\n\nEl valor de la hora del vendedor (este es un parámetro que debe estudiarse).\nEl costo de los peajes.\nEl costo del combustible.\n\nCada equipo debe definir el vehículo que utilizará el vendedor para realizar el recorrido y, con base en esta elección, calcular el costo del combustible.\n\n\n\n\nRepresentación Visual:\n\nCree un GIF animado o un video que muestre cómo se comporta la mejor solución encontrada, usando un gráfico del recorrido en el mapa de México.\n\n\n\n\nDiscusión:\nReflexione sobre: - Los resultados obtenidos con las colonias de hormigas y los algoritmos genéticos. - Comparación de costos y tiempo de ejecución."
  },
  {
    "objectID": "posts/post-with-code/index.html#funciones-a-optimizar",
    "href": "posts/post-with-code/index.html#funciones-a-optimizar",
    "title": "Primer trabajo: Optimización heurística",
    "section": "",
    "text": "Función de RosenbrockFunción de RastriginFunción de SchwefelFunción de GriewankFunción Goldstein-PriceFunción de las seis jorobas de camello\n\n\n\\[f(\\mathbf{x}) = \\sum_{i=1}^{d-1} \\left[ 100(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \\right]\\]\nEn 2 dimensiones se puede definir como \\[ f(x_1, x_2) = (a - x_1)^2 + b(x_2 - x_1^2)^2 \\]\nLa Función de Rosenbrock, también conocida como función del valle o del plátano, es ampliamente utilizada para evaluar algoritmos de optimización basados en gradientes. Esta función es unimodal y presenta su mínimo global en un valle parabólico estrecho, lo que facilita su localización. Sin embargo, segun Simon Fraser University (n.d.) citando a Picheny, Wagner, and Ginsbourger (2012) convergencia hacia este mínimo puede ser desafiante debido a la naturaleza del valle.\nLa función se evalúa generalmente en el hipercubo \\(x_i \\in [-5, 10]\\) y tiene un mínimo global en \\(f(1,...,1) = 0\\)\n\n\n\\[f(\\mathbf{x}) = 10d + \\sum_{i=1}^{d} \\left[ x_i^2 - 10 \\cos(2\\pi x_i) \\right]\\]\nSegun (simonfraser_rosenbroc?), la función de Rastrigin tiene varios mínimos locales. Es altamente multimodal, pero las ubicaciones de los mínimos se distribuyen regularmente. La función generalmente se evalúa en el hipercubo \\(x_i \\in [-5.12, 5.12]\\) y su mínimo local se encuentra en \\(f(0,...,0)=0\\).\n\n\n\\[ f(\\mathbf{x}) = 418.9829n - \\sum_{i=1}^{n} x_i \\sin(\\sqrt{|x_i|}) \\]\nSegun Simon Fraser University (n.d.) La función de Schwefel es compleja, con muchos mínimos locales. Normalmente se evalua en el hipercubo $x_i $. Su minimo global está en \\(f(420.9687,...,420.9687)=0\\)\n\n\n\\[ f(\\mathbf{x}) = 1 + \\frac{1}{4000} \\sum_{i=1}^{n} x_i^2 - \\prod_{i=1}^{n} \\cos\\left(\\frac{x_i}{\\sqrt{i}}\\right) \\]\nSegun Simon Fraser University (n.d.) la función de Griewank tiene muchos mínimos locales generalizados, que se distribuyen de forma regular. Lo que hace compleja su optimización al minimo global. Normalmente se evalua en el hipercubo $x_i $. Su minimo global está en \\(f(0,...,0)=0\\)\n\n\n\\[\n\\begin{align}\nf(x_1, x_2) = & \\left[1 + (x_1 + x_2 + 1)^2 (19 - 14x_1 + 3x_1^2 - 14x_2 + 6x_1x_2 + 3x_2^2)\\right] \\\\\n         & \\left[30 + (2x_1 - 3x_2)^2 (18 - 32x_1 + 12x_1^2 + 48x_2 - 36x_1x_2 + 27x_2^2)\\right]\n\\end{align}\n\\]\nLa función Goldstein-Price es una función en 2 dimensiones y tiene varios mínimos locales. Segun Molga and Smutnicki (2005), la función generalmente se evalúa en el cuadrado \\(x_1 \\in [-2, 2]\\) y \\(x_1 \\in [-2, 2]\\) . Su mínimo global es \\(f(0,-1) = 3\\)\n\n\n\\[ f(x_1, x_2) = \\left(4 - 2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2 \\]\nLa función de las seis jorobas de camello es una función en 2 dimensiones.Segun Molga and Smutnicki (2005) la función tiene seis mínimos locales, dos de los cuales son globales y recomienda evaluar la función en el rectángulo \\(x_1 \\in [-3, 3], x_2 \\in [-2, 2]\\), donde los mínimos globales son \\(f(0.0898,-0.7126) = -1.0316\\) y \\(f(-0.0898, 0.7126) = -1.0316\\)"
  },
  {
    "objectID": "posts/post-with-code/index.html#proceso-de-optimización",
    "href": "posts/post-with-code/index.html#proceso-de-optimización",
    "title": "Primer trabajo: Optimización heurística",
    "section": "",
    "text": "Función de RosenbrockFunción de SchwefelFunción de GriewankFunción Goldstein-PriceFunción de las seis jorobas de camello\n\n\n\\[ f(x_1, x_2) = (a - x_1)^2 + b(x_2 - x_1^2)^2 \\]\n\n\n\\[ f(x_1,x_2) = 418.9829n - \\sum_{i=1}^{2} x_i \\sin(\\sqrt{|x_i|}) \\]\n\n\n\\[ f(x_1,x_2) = 1 + \\frac{1}{4000} \\sum_{i=1}^{2} x_i^2 - \\prod_{i=1}^{2} \\cos\\left(\\frac{x_i}{\\sqrt{i}}\\right) \\]\n\n\n\\[\n\\begin{align}\nf(x_1, x_2) = & \\left[1 + (x_1 + x_2 + 1)^2 (19 - 14x_1 + 3x_1^2 - 14x_2 + 6x_1x_2 + 3x_2^2)\\right] \\\\\n         & \\left[30 + (2x_1 - 3x_2)^2 (18 - 32x_1 + 12x_1^2 + 48x_2 - 36x_1x_2 + 27x_2^2)\\right]\n\\end{align}\n\\]\n\n\n\\[ f(x_1, x_2) = \\left(4 - 2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2 \\]\n\n\n\n\n\n\n\nEscoja dos funciones de prueba.\nOptimización con método de descenso por gradiente:\n\nOptimice las funciones seleccionadas en dos y tres dimensiones usando un método de descenso por gradiente con condición inicial aleatoria.\n\nOptimización con métodos heurísticos:\n\nOptimice las funciones seleccionadas en dos y tres dimensiones usando:\n\nAlgoritmos evolutivos.\nOptimización de partículas.\nEvolución diferencial.\n\n\nRepresentación visual:\n\nCree un GIF animado o un video que muestre el proceso de optimización usando:\n\nDescenso por gradiente.\nMétodos heurísticos.\n\n\n\n\n\n\nReflexione sobre los siguientes puntos: - ¿Qué aportaron los métodos de descenso por gradiente y qué aportaron los métodos heurísticos? - Para responder a esta pregunta, considere: - El valor final de la función objetivo. - El número de evaluaciones de la función objetivo. - Es posible que se requiera realizar varias corridas de los algoritmos para obtener conclusiones significativas."
  },
  {
    "objectID": "posts/post-with-code/funciones.html",
    "href": "posts/post-with-code/funciones.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nMatplotlib is building the font cache; this may take a moment.\n\n\n\n\nCode\nimport numpy as np\n\n# Función de Rosenbrock\ndef rosenbrock(x1, x2, a=1, b=100):\n    return (a - x1)**2 + b * (x2 - x1**2)**2\n\n# Función de Schwefel\ndef schwefel(x1, x2):\n    return 418.9829 * 2 - (x1 * np.sin(np.sqrt(np.abs(x1))) + x2 * np.sin(np.sqrt(np.abs(x2))))\n\n# Función de Griewank\ndef griewank(x1, x2):\n    return 1 + (x1**2 + x2**2) / 4000 - (np.cos(x1 / np.sqrt(1)) * np.cos(x2 / np.sqrt(2)))\n\n# Función Goldstein-Price\ndef goldstein_price(x1, x2):\n    term1 = 1 + (x1 + x2 + 1)**2 * (19 - 14*x1 + 3*x1**2 - 14*x2 + 6*x1*x2 + 3*x2**2)\n    term2 = 30 + (2*x1 - 3*x2)**2 * (18 - 32*x1 + 12*x1**2 + 48*x2 - 36*x1*x2 + 27*x2**2)\n    return term1 * term2\n\n# Función de las seis jorobas de camello\ndef six_hump_camel(x1, x2):\n    return (4 - 2.1*x1**2 + (x1**4) / 3) * x1**2 + x1 * x2 + (-4 + 4*x2**2) * x2**2\n\n\n\n\nCode\ndef plot_function(f, x1_range, x2_range, title=\"Function Plot\"):\n    x1 = np.linspace(x1_range[0], x1_range[1], 400)\n    x2 = np.linspace(x2_range[0], x2_range[1], 400)\n    X1, X2 = np.meshgrid(x1, x2)\n    Z = f(X1, X2)\n\n    fig = plt.figure(figsize=(14, 6))\n\n    # 3D plot\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax1.plot_surface(X1, X2, Z)\n    ax1.set_title(f'3D Plot of {title}')\n    ax1.set_xlabel('X1')\n    ax1.set_ylabel('X2')\n    ax1.set_zlabel('Z')\n\n    # Contour plot\n    ax2 = fig.add_subplot(122)\n    contour = ax2.contour(X1, X2, Z)\n    ax2.set_title(f'Contour Plot of {title}')\n    ax2.set_xlabel('X1')\n    ax2.set_ylabel('X2')\n    fig.colorbar(contour, ax=ax2)\n\n    plt.show()\n\ndef plot_function(f, x1_range, x2_range, title=\"Function Plot\", x1_point=None, x2_point=None):\n    x1 = np.linspace(x1_range[0], x1_range[1], 400)\n    x2 = np.linspace(x2_range[0], x2_range[1], 400)\n    X1, X2 = np.meshgrid(x1, x2)\n    Z = f(X1, X2)\n\n    fig = plt.figure(figsize=(14, 6))\n\n    # 3D plot\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax1.plot_surface(X1, X2, Z)\n    ax1.set_title(f'3D Plot of {title}')\n    ax1.set_xlabel('X1')\n    ax1.set_ylabel('X2')\n    ax1.set_zlabel('Z')\n\n    if x1_point is not None and x2_point is not None:\n        z_point = f(x1_point, x2_point)\n        ax1.scatter(x1_point, x2_point, z_point+1, color='red', s=50)\n\n    # Contour plot\n    ax2 = fig.add_subplot(122)\n    contour = ax2.contour(X1, X2, Z, cmap='viridis')\n    ax2.set_title(f'Contour Plot of {title}')\n    ax2.set_xlabel('X1')\n    ax2.set_ylabel('X2')\n    fig.colorbar(contour, ax=ax2)\n\n    if x1_point is not None and x2_point is not None:\n        ax2.scatter(x1_point, x2_point, color='red', s=50)\n\n    plt.show()\n\n# Ejemplo de uso con la función de Rosenbrock\ndef rosenbrock(x1, x2, a=1, b=100):\n    return (a - x1)**2 + b * (x2 - x1**2)**2\n\nplot_function(rosenbrock, x1_range=(-2, 2), x2_range=(-1, 3), title=\"Rosenbrock Function\", x1_point=-1, x2_point=1)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\ndef plot_function(f, x1_range, x2_range, title=\"Function Plot\", x1_point=None, x2_point=None):\n    x1 = np.linspace(x1_range[0], x1_range[1], 400)\n    x2 = np.linspace(x2_range[0], x2_range[1], 400)\n    X1, X2 = np.meshgrid(x1, x2)\n    Z = f(X1, X2)\n\n    fig = plt.figure(figsize=(14, 6))\n\n    \n    # 3D plot\n    ax1 = fig.add_subplot(121, projection='3d')\n    if x1_point is not None and x2_point is not None:\n        z_point = f(x1_point, x2_point)\n        ax1.scatter(x1_point, x2_point, z_point, color='red', s=50, depthshade=False)  # Ajuste pequeño\n    ax1.plot_surface(X1, X2, Z, cmap='viridis', alpha=0.7)\n    ax1.set_title(f'3D Plot of {title}')\n    ax1.set_xlabel('X1')\n    ax1.set_ylabel('X2')\n    ax1.set_zlabel('Z')\n\n   \n\n    # Contour plot\n    ax2 = fig.add_subplot(122)\n    contour = ax2.contour(X1, X2, Z, cmap='viridis')\n    ax2.set_title(f'Contour Plot of {title}')\n    ax2.set_xlabel('X1')\n    ax2.set_ylabel('X2')\n    fig.colorbar(contour, ax=ax2)\n\n    if x1_point is not None and x2_point is not None:\n        ax2.scatter(x1_point, x2_point, color='red', s=50)\n\n    plt.show()\n\n# Ejemplo de uso con la función de Rosenbrock\ndef rosenbrock(x1, x2, a=1, b=100):\n    return (a - x1)**2 + b * (x2 - x1**2)**2\n\nplot_function(rosenbrock, x1_range=(-2, 2), x2_range=(-1, 3), title=\"Rosenbrock Function\", x1_point=-1, x2_point=1)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef plot_function(f, x1_range, x2_range, title=\"Function Plot\", x1_point=None, x2_point=None):\n    x1 = np.linspace(x1_range[0], x1_range[1], 400)\n    x2 = np.linspace(x2_range[0], x2_range[1], 400)\n    X1, X2 = np.meshgrid(x1, x2)\n    Z = f(X1, X2)\n\n    fig = plt.figure(figsize=(14, 6))\n\n    # 3D plot\n    ax1 = fig.add_subplot(121, projection='3d')\n    # Primero dibujamos la superficie\n    surface = ax1.plot_surface(X1, X2, Z, cmap='viridis', alpha=0.7)\n    \n    # Luego dibujamos el punto\n    if x1_point is not None and x2_point is not None:\n        z_point = f(x1_point, x2_point)\n        ax1.scatter(x1_point, x2_point, z_point, color='red', s=100, depthshade=False, linewidth=2, edgecolor='black')\n    \n    ax1.set_title(f'3D Plot of {title}')\n    ax1.set_xlabel('X1')\n    ax1.set_ylabel('X2')\n    ax1.set_zlabel('Z')\n\n\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef plot_function_3d(f, x1_range, x2_range, title=\"Function Plot\", x1_point=None, x2_point=None):\n    # Create the mesh grid\n    x1 = np.linspace(x1_range[0], x1_range[1], 100)\n    x2 = np.linspace(x2_range[0], x2_range[1], 100)\n    X1, X2 = np.meshgrid(x1, x2)\n    Z = f(X1, X2)\n    \n    # Create subplots\n    fig = make_subplots(\n        rows=1, cols=2,\n        specs=[[{'type': 'surface'}, {'type': 'contour'}]],\n        subplot_titles=('3D Surface Plot', 'Contour Plot')\n    )\n    \n    # Add surface plot\n    fig.add_trace(\n        go.Surface(x=X1, y=X2, z=Z, colorscale='viridis', opacity=0.8),\n        row=1, col=1\n    )\n    \n    # Add point if specified\n    if x1_point is not None and x2_point is not None:\n        z_point = f(x1_point, x2_point)\n        fig.add_trace(\n            go.Scatter3d(\n                x=[x1_point],\n                y=[x2_point],\n                z=[z_point],\n                mode='markers',\n                marker=dict(size=8, color='red'),\n                name='Point'\n            ),\n            row=1, col=1\n        )\n    \n    # Add contour plot\n    fig.add_trace(\n        go.Contour(\n            x=x1,\n            y=x2,\n            z=Z,\n            colorscale='viridis'\n        ),\n        row=1, col=2\n    )\n    \n    # Update layout\n    fig.update_layout(\n        title=title,\n        width=1200,\n        height=500,\n        scene=dict(\n            xaxis_title='X1',\n            yaxis_title='X2',\n            zaxis_title='Z'\n        )\n    )\n    \n    fig.show()\n\n# Test the function\ndef rosenbrock(x1, x2, a=1, b=100):\n    return (a - x1)**2 + b * (x2 - x1**2)**2\n\nplot_function_3d(rosenbrock, (-2, 2), (-1, 3), \"Rosenbrock Function\", x1_point=-1, x2_point=1)\n\nplot_function(rosenbrock, x1_range=(-5, 5), x2_range=(-5, 15), title=\"Rosenbrock Function\",x1_point=-1, x2_point=1)\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[64], line 2\n      1 import numpy as np\n----&gt; 2 import plotly.graph_objects as go\n      3 from plotly.subplots import make_subplots\n      5 def plot_function_3d(f, x1_range, x2_range, title=\"Function Plot\", x1_point=None, x2_point=None):\n      6     # Create the mesh grid\n\nModuleNotFoundError: No module named 'plotly'"
  }
]