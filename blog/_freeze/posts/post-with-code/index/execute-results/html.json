{
  "hash": "cb3a5fe894673a20517bbda23f622ff6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Primer trabajo: Optimización heurística\"\nauthor:\n  - name: \"Catalina Restrepo Salgado\"\n  - name: \"Julián Castaño Pineda\"\n  - name: \"Tomás Rodríguez Taborda\"\n  - name: \"Luis Andrés Altamar Romero\"\ndate: \"2024-11-15\"\ncategories: [optimización]\nimage: \"image.jpg\"\nbibliography: ref.bib\n---\n\n# Primera parte: Optimización Numérica\n\nEl objetivo de esta sección es evaluar diversos métodos de optimización aplicados a varias funciones, con el fin de medir su rendimiento. En particular, se utilizarán las funciones de Rosenbrock, Schwefel, Griewank, Goldstein-Price y la función de las seis jorobas de camello. Estas funciones serán optimizadas mediante el método del gradiente descendente y tres algoritmos heurísticos: Algoritmos Evolutivos, Optimización por Enjambre de Partículas y Evolución Diferencial.\n\nAl final, se comentará sobre los aportes de los métodos de descenso por gradiente y los métodos heurísticos, considerando el valor final de la función objetivo y el número de evaluaciones de la función objetivo, en un entorno de simulación con varios parámetros y condiciones para garantizar conclusiones significativas.\n\n## Funciones a optimizar\n\n::: panel-tabset\n### Función de Rosenbrock\n$$f(\\mathbf{x}) = \\sum_{i=1}^{d-1} \\left[ 100(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \\right]$$\n\nEn 2 dimensiones se puede definir como\n$$ f(x_1, x_2) = (a - x_1)^2 + b(x_2 - x_1^2)^2 $$\n\nLa Función de Rosenbrock, también conocida como función del valle o del plátano, es ampliamente utilizada para evaluar algoritmos de optimización basados en gradientes. Esta función es unimodal y presenta su mínimo global en un valle parabólico estrecho, lo que facilita su localización. Sin embargo, segun @simonfraser_rosenbrock citando a @picheny2012benchmark convergencia hacia este mínimo puede ser desafiante debido a la naturaleza del valle.\n\nLa función se evalúa generalmente en el hipercubo $x_i \\in [-5, 10]$\ny tiene un mínimo global en $f(1,...,1) = 0$ \n\n### Función de Rastrigin\n \n $$f(\\mathbf{x}) = 10d + \\sum_{i=1}^{d} \\left[ x_i^2 - 10 \\cos(2\\pi x_i) \\right]$$\n\nSegun @simonfraser_rosenbroc, la función de Rastrigin tiene varios mínimos locales. Es altamente multimodal, pero las ubicaciones de los mínimos se distribuyen regularmente. La función generalmente se evalúa en el hipercubo $x_i \\in [-5.12, 5.12]$ y su mínimo local se encuentra en $f(0,...,0)=0$.\n\n### Función de Schwefel\n\n$$ f(\\mathbf{x}) = 418.9829n - \\sum_{i=1}^{n} x_i \\sin(\\sqrt{|x_i|}) $$\n\nSegun @simonfraser_rosenbrock La función de Schwefel es compleja, con muchos mínimos locales. Normalmente se evalua en el hipercubo $x_i \\in [-500,500] $. Su minimo global está en $f(420.9687,...,420.9687)=0$\n\n### Función de Griewank\n$$ f(\\mathbf{x}) = 1 + \\frac{1}{4000} \\sum_{i=1}^{n} x_i^2 - \\prod_{i=1}^{n} \\cos\\left(\\frac{x_i}{\\sqrt{i}}\\right) $$\n\nSegun @simonfraser_rosenbrock la función de Griewank tiene muchos mínimos locales generalizados, que se distribuyen de forma regular. Lo que hace compleja su optimización al minimo global. Normalmente se evalua en el hipercubo $x_i \\in [-600,600] $. Su minimo global está en $f(0,...,0)=0$\n\n### Función Goldstein-Price\n\n\n$$\n\\begin{align}\nf(x_1, x_2) = & \\left[1 + (x_1 + x_2 + 1)^2 (19 - 14x_1 + 3x_1^2 - 14x_2 + 6x_1x_2 + 3x_2^2)\\right] \\\\\n         & \\left[30 + (2x_1 - 3x_2)^2 (18 - 32x_1 + 12x_1^2 + 48x_2 - 36x_1x_2 + 27x_2^2)\\right]\n\\end{align}\n$$\n\n\nLa función Goldstein-Price es una función en 2 dimensiones y tiene varios mínimos locales. Segun @molga2005test, la función generalmente se evalúa en el cuadrado $x_1 \\in [-2, 2]$ y $x_1 \\in [-2, 2]$ . Su mínimo global es $f(0,-1) = 3$\n\n### Función de las seis jorobas de camello\n\n $$ f(x_1, x_2) = \\left(4 - 2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2 $$\n\nLa función de las seis jorobas de camello es una función en 2 dimensiones.Segun @molga2005test la función tiene seis mínimos locales, dos de los cuales son globales y recomienda evaluar la función en el rectángulo $x_1 \\in [-3, 3], x_2 \\in [-2, 2]$, donde los mínimos globales son $f(0.0898,-0.7126) = -1.0316$ y $f(-0.0898, 0.7126) = -1.0316$\n\n:::\n\n## Proceso de optimización\n\n### Optimización por descenso del gradiente \n\n::: panel-tabset\n#### Función de Rosenbrock\n $$ f(x_1, x_2) = (a - x_1)^2 + b(x_2 - x_1^2)^2 $$\n\n#### Función de Schwefel\n$$ f(x_1,x_2) = 418.9829n - \\sum_{i=1}^{2} x_i \\sin(\\sqrt{|x_i|}) $$\n\n#### Función de Griewank\n$$ f(x_1,x_2) = 1 + \\frac{1}{4000} \\sum_{i=1}^{2} x_i^2 - \\prod_{i=1}^{2} \\cos\\left(\\frac{x_i}{\\sqrt{i}}\\right) $$\n\n#### Función Goldstein-Price\n\n$$\n\\begin{align}\nf(x_1, x_2) = & \\left[1 + (x_1 + x_2 + 1)^2 (19 - 14x_1 + 3x_1^2 - 14x_2 + 6x_1x_2 + 3x_2^2)\\right] \\\\\n         & \\left[30 + (2x_1 - 3x_2)^2 (18 - 32x_1 + 12x_1^2 + 48x_2 - 36x_1x_2 + 27x_2^2)\\right]\n\\end{align}\n$$\n\n#### Función de las seis jorobas de camello\n $$ f(x_1, x_2) = \\left(4 - 2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2 $$\n \n:::\n\n\n### Tareas:\n1. **Escoja dos funciones de prueba.**\n2. **Optimización con método de descenso por gradiente:**\n   - Optimice las funciones seleccionadas en **dos y tres dimensiones** usando un **método de descenso por gradiente** con condición inicial aleatoria.\n3. **Optimización con métodos heurísticos:**\n   - Optimice las funciones seleccionadas en **dos y tres dimensiones** usando:\n     - Algoritmos evolutivos.\n     - Optimización de partículas.\n     - Evolución diferencial.\n4. **Representación visual:**\n   - Cree un **GIF animado** o un **video** que muestre el proceso de optimización usando:\n     - **Descenso por gradiente**.\n     - **Métodos heurísticos**.\n\n### Discusión:\nReflexione sobre los siguientes puntos:\n- ¿Qué aportaron los métodos de **descenso por gradiente** y qué aportaron los **métodos heurísticos**?\n  - Para responder a esta pregunta, considere:\n    - El **valor final** de la función objetivo.\n    - El **número de evaluaciones** de la función objetivo.\n  - Es posible que se requiera realizar **varias corridas** de los algoritmos para obtener conclusiones significativas.\n\n\n# Parte 2: Optimización Combinatoria\n\n## Problema del Viajero:\nUn vendedor debe realizar un recorrido por **todas las capitales** de los **32 estados** de los **Estados Unidos Mexicanos**.\n\n### Tareas:\n1. **Optimización con métodos metaheurísticos:**\n   - Utilice **colonias de hormigas** para encontrar el orden óptimo del recorrido.\n   - Utilice **algoritmos genéticos** para encontrar el orden óptimo del recorrido.\n2. **Costo del recorrido:**\n   - El costo de desplazamiento entre ciudades se calcula como la suma de:\n     - El valor de la **hora del vendedor** (este es un parámetro que debe estudiarse).\n     - El **costo de los peajes**.\n     - El **costo del combustible**.\n   - Cada equipo debe definir el **vehículo** que utilizará el vendedor para realizar el recorrido y, con base en esta elección, **calcular el costo del combustible**.\n\n### Representación Visual:\n- Cree un **GIF animado** o un **video** que muestre cómo se comporta la **mejor solución** encontrada, usando un **gráfico del recorrido** en el mapa de México.\n\n---\n\n### Discusión:\nReflexione sobre:\n- Los resultados obtenidos con las **colonias de hormigas** y los **algoritmos genéticos**.\n- Comparación de costos y tiempo de ejecución.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}