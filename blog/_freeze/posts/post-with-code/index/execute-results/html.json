{
  "hash": "4c1ee527c6d3a5aa139e056b3a2244d6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Primer trabajo: Optimización heurística\"\nauthor:\n  - name: \"Catalina Restrepo Salgado\"\n  - name: \"Julián Castaño Pineda\"\n  - name: \"Tomás Rodríguez Taborda\"\n  - name: \"Luis Andrés Altamar Romero\"\ndate: \"2024-11-15\"\ncategories: [optimización]\nimage: \"image.jpg\"\n---\n\n::: {#7d4a27cd .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n# Primera parte: Optimización Numérica\n\nEl objetivo de esta sección es evaluar diversos métodos de optimización aplicados a varias funciones, con el fin de medir su rendimiento. En particular, se utilizarán las funciones de Rosenbrock, Schwefel, Griewank, Goldstein-Price y la función de las seis jorobas de camello. Estas funciones serán optimizadas mediante el método del gradiente descendente y tres algoritmos heurísticos: Algoritmos Evolutivos, Optimización por Enjambre de Partículas y Evolución Diferencial.\n\nAl final, se comentará sobre los aportes de los métodos de descenso por gradiente y los métodos heurísticos, considerando el valor final de la función objetivo y el número de evaluaciones de la función objetivo, en un entorno de simulación con varios parámetros y condiciones para garantizar conclusiones significativas.\n\n## Funciones a optimizar\n\n::: panel-tabset\n### Función de Rosenbrock\n $$ f(x_1, x_2) = (a - x_1)^2 + b(x_2 - x_1^2)^2 $$\n\n### Función de Schwefel\n$$ f(x_1,x_2) = 418.9829n - \\sum_{i=1}^{2} x_i \\sin(\\sqrt{|x_i|}) $$\n\n### Función de Griewank\n$$ f(x_1,x_2) = 1 + \\frac{1}{4000} \\sum_{i=1}^{2} x_i^2 - \\prod_{i=1}^{2} \\cos\\left(\\frac{x_i}{\\sqrt{i}}\\right) $$\n\n### Función Goldstein-Price\n\n$$\n\\begin{align}\nf(x_1, x_2) = & \\left[1 + (x + y + 1)^2 (19 - 14x + 3x^2 - 14y + 6xy + 3y^2)\\right] \\\\\n         & \\left[30 + (2x - 3y)^2 (18 - 32x + 12x^2 + 48y - 36xy + 27y^2)\\right]\n\\end{align}\n$$\n\n### Función de las seis jorobas de camello\n $$ f(x_1, x_2) = \\left(4 - 2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2 $$\n\n:::\n\n## Proceso de optimización\n\n### Optimización por descenso del gradiente \n\n::: panel-tabset\n#### Función de Rosenbrock\n $$ f(x, y) = (a - x)^2 + b(y - x^2)^2 $$\n\n::: {#fd0cbc44 .cell execution_count=2}\n``` {.python .cell-code}\n# Ejemplo de código Python\nimport numpy as np\n\ndef rosenbrock(x, y):\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\nx = np.linspace(-2, 2, 400)\ny = np.linspace(-1, 3, 400)\nX, Y = np.meshgrid(x, y)\nZ = rosenbrock(X, Y)\n\nprint(\"Valor mínimo de la función de Rosenbrock:\", np.min(Z))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValor mínimo de la función de Rosenbrock: 6.2853131372160005e-06\n```\n:::\n:::\n\n\n#### Función de Schwefel\n$$ f(\\mathbf{x}) = 418.9829n - \\sum_{i=1}^{n} x_i \\sin(\\sqrt{|x_i|}) $$\n\n#### Función de Griewank\n$$ f(\\mathbf{x}) = 1 + \\frac{1}{4000} \\sum_{i=1}^{n} x_i^2 - \\prod_{i=1}^{n} \\cos\\left(\\frac{x_i}{\\sqrt{i}}\\right) $$\n\n#### Función Goldstein-Price\n\n$$\n\\begin{align}\nf(x, y) = & \\left[1 + (x + y + 1)^2 (19 - 14x + 3x^2 - 14y + 6xy + 3y^2)\\right] \\\\\n         & \\left[30 + (2x - 3y)^2 (18 - 32x + 12x^2 + 48y - 36xy + 27y^2)\\right]\n\\end{align}\n$$\n\n#### Función de las seis jorobas de camello\n $$ f(x, y) = \\left(4 - 2.1x^2 + \\frac{x^4}{3}\\right)x^2 + xy + \\left(-4 + 4y^2\\right)y^2 $$\n\n:::\n\n### Tareas:\n1. **Escoja dos funciones de prueba.**\n2. **Optimización con método de descenso por gradiente:**\n   - Optimice las funciones seleccionadas en **dos y tres dimensiones** usando un **método de descenso por gradiente** con condición inicial aleatoria.\n3. **Optimización con métodos heurísticos:**\n   - Optimice las funciones seleccionadas en **dos y tres dimensiones** usando:\n     - Algoritmos evolutivos.\n     - Optimización de partículas.\n     - Evolución diferencial.\n4. **Representación visual:**\n   - Cree un **GIF animado** o un **video** que muestre el proceso de optimización usando:\n     - **Descenso por gradiente**.\n     - **Métodos heurísticos**.\n\n### Discusión:\nReflexione sobre los siguientes puntos:\n- ¿Qué aportaron los métodos de **descenso por gradiente** y qué aportaron los **métodos heurísticos**?\n  - Para responder a esta pregunta, considere:\n    - El **valor final** de la función objetivo.\n    - El **número de evaluaciones** de la función objetivo.\n  - Es posible que se requiera realizar **varias corridas** de los algoritmos para obtener conclusiones significativas.\n\n\n# Parte 2: Optimización Combinatoria\n\n## Problema del Viajero:\nUn vendedor debe realizar un recorrido por **todas las capitales** de los **32 estados** de los **Estados Unidos Mexicanos**.\n\n### Tareas:\n1. **Optimización con métodos metaheurísticos:**\n   - Utilice **colonias de hormigas** para encontrar el orden óptimo del recorrido.\n   - Utilice **algoritmos genéticos** para encontrar el orden óptimo del recorrido.\n2. **Costo del recorrido:**\n   - El costo de desplazamiento entre ciudades se calcula como la suma de:\n     - El valor de la **hora del vendedor** (este es un parámetro que debe estudiarse).\n     - El **costo de los peajes**.\n     - El **costo del combustible**.\n   - Cada equipo debe definir el **vehículo** que utilizará el vendedor para realizar el recorrido y, con base en esta elección, **calcular el costo del combustible**.\n\n### Representación Visual:\n- Cree un **GIF animado** o un **video** que muestre cómo se comporta la **mejor solución** encontrada, usando un **gráfico del recorrido** en el mapa de México.\n\n---\n\n### Discusión:\nReflexione sobre:\n- Los resultados obtenidos con las **colonias de hormigas** y los **algoritmos genéticos**.\n- Comparación de costos y tiempo de ejecución.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}